{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb37e65",
   "metadata": {},
   "source": [
    "# Rossmann Sales Forecasting â€” Time Series Modeling\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Rossmann operates over 3,000 drugstores across Europe. Due to the short shelf life of many pharmaceutical products, it's essential to forecast daily sales accurately.\n",
    "\n",
    "Currently, store managers manually forecast daily sales for the next six weeks. To improve consistency and accuracy, we're tasked with building a **data-driven time-series model** to automate this process.\n",
    "\n",
    "---\n",
    "\n",
    "### Objective:\n",
    "\n",
    "Build a robust **time-series forecasting pipeline** to predict **daily sales for the next 6 weeks**, for **9 key Rossmann stores** using:\n",
    "- Time-series decomposition\n",
    "- Stationarity checks (ADF test)\n",
    "- Cointegration tests (Johansen)\n",
    "- VAR or VARMAX modeling\n",
    "- MAPE as evaluation metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d794b73e",
   "metadata": {},
   "source": [
    "## 2. Load Data & Basic Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8896c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from statsmodels.tsa.api import VARMAX\n",
    "from statsmodels.tsa.stattools import ccf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ee4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_option to see maximum data\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "pd.set_option(\"max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5590be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSVs\n",
    "train_df = pd.read_csv(r\"C:\\Users\\lucky\\Desktop\\Capstone Project\\Dataset\\train.csv\", parse_dates=[\"Date\"])\n",
    "store_df = pd.read_csv(r\"C:\\Users\\lucky\\Desktop\\Capstone Project\\Dataset\\store.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb481511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data shapes\n",
    "print(\"Train data shape:\", train_df.shape)\n",
    "print(\"Store data shape:\", store_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee900c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Samples of train data and Store data\n",
    "\n",
    "print(\"-----------  Train dataset Sample  ----------\")\n",
    "print(train_df.head())\n",
    "print(\"-\"*100)\n",
    "print(\"-----------  Store dataset Sample  ----------\")\n",
    "print(store_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6169b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the datatype of the columns in the \"Train\" and \"Store\" dataset\n",
    "\n",
    "print(\"----------  Train Data Types  ----------\")\n",
    "print(train_df.info())\n",
    "print(\"-\"*100)\n",
    "print(\"----------  Store Data Types  ----------\")\n",
    "print(store_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b78b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the summary of the \"Train\" and \"Store\" dataset\n",
    "\n",
    "print(\"----------  Summary of Train Datasets  ----------\")\n",
    "print(train_df.describe())\n",
    "print(\"-\"*100)\n",
    "print(\"----------  Summary of Store Datasets  ----------\")\n",
    "print(store_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b27e6",
   "metadata": {},
   "source": [
    "#### Dataset Cleaning and Dataset Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6169618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking percentage of missing/null values\n",
    "\n",
    "print(\"-----  Percentage of Missing/null values in Store Dataset  -----\")\n",
    "print(round(100*(store_df.isnull().sum()/len(store_df)),2))\n",
    "print(\"-\"*100)\n",
    "print(\"-----  Percentage of Missing/null values in Train Dataset  -----\")\n",
    "print(round(100*(train_df.isnull().sum()/len(train_df)),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018a321",
   "metadata": {},
   "source": [
    "## 3. Dataset Cleaning and Transformation\n",
    "\n",
    "In this step, we are going to clean and prepare the data by:\n",
    "\n",
    "- Merging training and store metadata\n",
    "- Focusing on a subset of selected stores\n",
    "- Handling missing values appropriately\n",
    "- Creating a new feature (`Promo2Active`) to reflect ongoing promotions\n",
    "- Fixing logical inconsistencies in `CompetitionDistance`\n",
    "- Sorting the data by `Store` and `Date` for time-series modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080fe14",
   "metadata": {},
   "source": [
    "### **Load and Merge Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee39a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge training and store metadata\n",
    "\n",
    "df = pd.merge(train_df, store_df, on='Store', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435b52b",
   "metadata": {},
   "source": [
    "### **Filter for Selected Stores**\n",
    "\n",
    "##### Focusing on 9 Key Stores for Modeling\n",
    "\n",
    "The company has decided to focus only on 9 key stores across Europe. These stores have been selected based on their high revenue contributions and strategic importance to the business. \n",
    "\n",
    "By narrowing the scope to these specific stores, we can build more targeted and efficient forecasting models. Once the modeling approach is validated and performs well for these stores, it can be scaled to include the remaining stores in the future.\n",
    "\n",
    "The selected stores for this case study are: **Store 1, 3, 8, 9, 13, 25, 29, 31, and 46**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a102c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on 9 selected stores for time-series modeling\n",
    "\n",
    "selected_stores = [1, 3, 8, 9, 13, 25, 29, 31, 46]\n",
    "df = df[df[\"Store\"].isin(selected_stores)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5380693f",
   "metadata": {},
   "source": [
    "### **Drop Leakage Columns and Handle Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac8acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Customers' column â€” it's highly correlated with Sales and not known in advance\n",
    "\n",
    "df.drop(columns=[\"Customers\"], inplace=True)\n",
    "\n",
    "\n",
    "# Fill NA values in specified competition/promo columns with 0\n",
    "\n",
    "comp_cols = [\"CompetitionDistance\", \"CompetitionOpenSinceMonth\", \"CompetitionOpenSinceYear\", \n",
    "             \"Promo2SinceWeek\", \"Promo2SinceYear\", \"PromoInterval\"]\n",
    "\n",
    "df[comp_cols] = df[comp_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dad399",
   "metadata": {},
   "source": [
    "### **Fix Logical Inconsistencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dad7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is no competition, opening month/year should be zero as well\n",
    "\n",
    "df.loc[df[\"CompetitionDistance\"] == 0, \"CompetitionOpenSinceMonth\"] = 0\n",
    "df.loc[df[\"CompetitionDistance\"] == 0, \"CompetitionOpenSinceYear\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1447a1",
   "metadata": {},
   "source": [
    "### **Feature Engineering - Promo2 Active Flag**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660c1cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Date is datetime type\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "\n",
    "# Helper to determine if Promo2 is active in the current month\n",
    "\n",
    "def is_promo2_active(row):\n",
    "    if row[\"Promo2\"] == 1 and row[\"PromoInterval\"] != 0:\n",
    "        promo_months = row[\"PromoInterval\"].split(\",\")\n",
    "        return row[\"Date\"].strftime(\"%b\") in promo_months\n",
    "    return False\n",
    "\n",
    "df[\"Promo2Active\"] = df.apply(is_promo2_active, axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c9c1d",
   "metadata": {},
   "source": [
    "### **Final Sort and Reset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd32aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by Store and Date for sequential modeling\n",
    "\n",
    "df.sort_values(by=[\"Store\", \"Date\"], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc697b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final shape and sample\n",
    "print(\"Merged Dataset Shape:- \",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f900a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cleaned sample dataset\n",
    "print(\"-----------  Sample Merged Dataset  ---------- \")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Datasets\n",
    "print(\"----------  Summary of Merged Datasets  ----------\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b777d4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged datatypes\n",
    "print(\"----------  Information of Merged Datasets  ----------\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b522859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking percentage of missing/null values\n",
    "print(\"----------  Percentage of Missing/null Values of Merged Datasets ----------\")\n",
    "round(100*(df.isnull().sum()/len(df)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340ebfa",
   "metadata": {},
   "source": [
    "## 4. Outlier Removal & Skewness Check\n",
    "\n",
    "##### Outlier Capping and Skewness Check\n",
    "\n",
    "Outliers, especially in the upper range of sales, can heavily skew time-series models. To mitigate this, we cap extreme values above the 99th percentile for each store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c602140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers above 99th percentile of Sales per store\n",
    "for store in selected_stores:\n",
    "    p99 = df[df.Store == store][\"Sales\"].quantile(0.99)\n",
    "    df.loc[(df.Store == store) & (df[\"Sales\"] > p99), \"Sales\"] = p99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8845b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skewness\n",
    "sns.histplot(df[\"Sales\"], kde=True)\n",
    "plt.title(\"Sales Distribution after Outlier Removal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6733a9",
   "metadata": {},
   "source": [
    "Additionally, we observed many zero-sales days. Upon investigation, these are mostly days when the store was closed. Since no sales can occur when the store is closed, we remove those rows (where `Open = 0`) to avoid skewing the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f143181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove closed days (they contribute to many zero sales)\n",
    "df = df[df[\"Open\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5951a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skewness\n",
    "sns.histplot(df[\"Sales\"], kde=True)\n",
    "plt.title(\"Sales Distribution after Outlier Removal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e576395b",
   "metadata": {},
   "source": [
    "The updated sales distribution plot now reflects realistic active sales, which helps in improving model accuracy and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d852d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting outliers at 90, 95 and 99 percentile\n",
    "\n",
    "df.describe(percentiles=[0.90,0.95,0.99]).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976d7aae",
   "metadata": {},
   "source": [
    "- Outliers at the 99th percentile can skew the time series model drastically.\n",
    "- We remove extreme sales values to ensure the stationarity test and variance structure remain valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeaeea9",
   "metadata": {},
   "source": [
    "## 5. Stratified Train-Test Split (Per Store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f399c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split last 6 weeks as test set (per store)\n",
    "df[\"is_test\"] = 0\n",
    "\n",
    "for store in selected_stores:\n",
    "    store_data = df[df.Store == store]\n",
    "    cutoff_date = store_data[\"Date\"].max() - pd.Timedelta(days=42)\n",
    "    df.loc[(df.Store == store) & (df[\"Date\"] > cutoff_date), \"is_test\"] = 1\n",
    "\n",
    "# Confirm split\n",
    "df[\"is_test\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2667a",
   "metadata": {},
   "source": [
    "- Since this is time series, a temporal split ensures the model is trained on past data and tested on future unseen periods.\n",
    "- By maintaining a 6-week window for each store, we simulate real-world forecasting conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700f0f7",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis (EDA) Per Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“ˆ Plot Sales time series for each store\n",
    "for store in selected_stores:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    store_data = df[df.Store == store]\n",
    "    plt.plot(store_data[\"Date\"], store_data[\"Sales\"], label=f\"Store {store}\")\n",
    "    plt.title(f\" Sales over Time - Store {store}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Sales\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f94ca",
   "metadata": {},
   "source": [
    "Visualizing each storeâ€™s sales gives us:\n",
    "- A sense of trend and seasonality\n",
    "- Check for visible change points or anomalies\n",
    "- Guide whether decomposition or transformation is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21ff1c6",
   "metadata": {},
   "source": [
    "## 7. Seasonal Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f967049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "for store in selected_stores:\n",
    "    store_df = df[(df.Store == store) & (df.Open == 1)]\n",
    "    store_ts = store_df.set_index(\"Date\")[\"Sales\"].asfreq(\"D\").fillna(method=\"ffill\")\n",
    "\n",
    "    stl = STL(store_ts, seasonal=7)\n",
    "    result = stl.fit()\n",
    "\n",
    "    result.plot()\n",
    "    plt.suptitle(f\"STL Decomposition - Store {store}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12325b3a",
   "metadata": {},
   "source": [
    "- Helps visualize **trend**, **seasonality**, and **residuals**.\n",
    "- Identifies whether deseasonalizing is needed before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f17a153",
   "metadata": {},
   "source": [
    "## 8. Augmented Dickey-Fuller (ADF) Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff686f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(series, title=''):\n",
    "    \"\"\"\n",
    "    Perform ADF Test and print result.\n",
    "    \"\"\"\n",
    "    print(f\"\\n ADF Test: {title}\")\n",
    "    result = adfuller(series.dropna(), autolag='AIC')\n",
    "    labels = ['ADF Test Statistic', 'p-value', '#Lags Used', 'Num Observations Used']\n",
    "    for value, label in zip(result, labels):\n",
    "        print(f\"{label} : {value}\")\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Series is Stationary\")\n",
    "    else:\n",
    "        print(\"Series is Non-Stationary\")\n",
    "\n",
    "# Apply ADF Test on raw sales per store\n",
    "for store in selected_stores:\n",
    "    print(f\"\\nðŸ“ Store {store}\")\n",
    "    store_df = df[(df.Store == store) & (df.Open == 1)]\n",
    "    store_sales = store_df.set_index(\"Date\")[\"Sales\"].asfreq(\"D\").fillna(method=\"ffill\")\n",
    "    adf_test(store_sales, f\"Store {store} - Sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b9725e",
   "metadata": {},
   "source": [
    "- Checks for stationarity in the series.\n",
    "- Stationarity is a key assumption in most time-series models like VAR/VARMAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2389da",
   "metadata": {},
   "source": [
    "## 9. Differencing (If Non-Stationary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976864ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply differencing where needed and re-test\n",
    "store_diff_dict = {}\n",
    "\n",
    "for store in selected_stores:\n",
    "    print(f\"\\n Store {store}\")\n",
    "    store_df = df[(df.Store == store) & (df.Open == 1)]\n",
    "    store_sales = store_df.set_index(\"Date\")[\"Sales\"].asfreq(\"D\").fillna(method=\"ffill\")\n",
    "\n",
    "    adf_result = adfuller(store_sales.dropna(), autolag='AIC')[1]\n",
    "    if adf_result > 0.05:\n",
    "        # First-order differencing\n",
    "        store_sales_diff = store_sales.diff().dropna()\n",
    "        print(\" Applying First Differencing\")\n",
    "        adf_test(store_sales_diff, f\"Store {store} - Sales (1st diff)\")\n",
    "        store_diff_dict[store] = store_sales_diff\n",
    "    else:\n",
    "        store_diff_dict[store] = store_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9c5fb",
   "metadata": {},
   "source": [
    "### Differencing\n",
    "\n",
    "- Transforms a non-stationary series to stationary.\n",
    "- VAR requires stationarity; this is a necessary step before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9393fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "def johansen_test(df_subset, store):\n",
    "    \"\"\"\n",
    "    Apply Johansen Test on Sales and Promo2Active series.\n",
    "    Checks for cointegration to determine if the variables have a long-term equilibrium relationship.\n",
    "    \"\"\"\n",
    "    df_subset = df_subset.set_index(\"Date\")[[\"Sales\", \"Promo2Active\"]].asfreq(\"D\").fillna(method=\"ffill\")\n",
    "\n",
    "    # Skip if too few observations\n",
    "    if len(df_subset) < 150:\n",
    "        print(f\"Skipping Store {store}: Not enough data points.\")\n",
    "        return\n",
    "\n",
    "    # Skip if any column has no variation\n",
    "    if df_subset[\"Sales\"].nunique() < 2 or df_subset[\"Promo2Active\"].nunique() < 2:\n",
    "        print(f\"Skipping Store {store}: One or more series has no variation.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        coint_result = coint_johansen(df_subset, det_order=0, k_ar_diff=1)\n",
    "        print(f\"\\nJohansen Test - Store {store}\")\n",
    "        trace_stat = coint_result.lr1\n",
    "        critical_values = coint_result.cvt[:, 1]  # 5% significance\n",
    "        for i, (stat, crit) in enumerate(zip(trace_stat, critical_values)):\n",
    "            print(f\"Rank {i}: Trace Stat = {stat:.2f}, Crit Value (5%) = {crit}\")\n",
    "            if stat > crit:\n",
    "                print(\"  âž¤ Cointegration Exists at Rank\", i)\n",
    "            else:\n",
    "                print(\"  âœ– No Cointegration at Rank\", i)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Johansen Test for Store {store}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43803965",
   "metadata": {},
   "outputs": [],
   "source": [
    "for store in selected_stores:\n",
    "    store_data = df[(df.Store == store) & (df.Open == 1)].copy()\n",
    "    johansen_test(store_data, store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e12808",
   "metadata": {},
   "source": [
    "## 10. Johansen Cointegration Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed1192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "def johansen_test(df_subset, store):\n",
    "    \"\"\"\n",
    "    Apply Johansen Test on multiple series (Sales + Promo2Active).\n",
    "    \"\"\"\n",
    "    df_subset = df_subset.set_index(\"Date\")[[\"Sales\", \"Promo2Active\"]].asfreq(\"D\").fillna(method=\"ffill\")\n",
    "    coint_result = coint_johansen(df_subset, det_order=0, k_ar_diff=1)\n",
    "\n",
    "    print(f\"\\n Johansen Test - Store {store}\")\n",
    "    trace_stat = coint_result.lr1\n",
    "    critical_values = coint_result.cvt[:, 1]  # 5% significance\n",
    "    for i, (stat, crit) in enumerate(zip(trace_stat, critical_values)):\n",
    "        print(f\"Rank {i}: Trace Stat = {stat:.2f}, Crit Value (5%) = {crit}\")\n",
    "        if stat > crit:\n",
    "            print(\" Cointegration Exists at Rank\", i)\n",
    "        else:\n",
    "            print(\" No Cointegration at Rank\", i)\n",
    "\n",
    "# Run Johansen test per store\n",
    "for store in selected_stores:\n",
    "    print(f\"\\n Store {store}\")\n",
    "    store_data = df[(df.Store == store) & (df.Open == 1)].copy()\n",
    "    johansen_test(store_data, store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d73962b",
   "metadata": {},
   "source": [
    "### Johansen Test\n",
    "\n",
    "- Required when building VAR/VARMAX models on multivariate non-stationary series.\n",
    "- Determines if variables (like Sales and Promo2Active) are cointegrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dd21f7",
   "metadata": {},
   "source": [
    "## 11. Model Building & Forecasting Loop (Per Store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d541d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build VAR/VARMAX model, forecast, and evaluate per store\n",
    "forecast_results = []\n",
    "\n",
    "for store in selected_stores:\n",
    "    print(f\"\\n Building Model for Store {store}\")\n",
    "    \n",
    "    #  Get train and test data\n",
    "    store_data = df[(df.Store == store) & (df.Open == 1)].copy()\n",
    "    store_data.set_index(\"Date\", inplace=True)\n",
    "    \n",
    "    train = store_data[store_data[\"is_test\"] == 0]\n",
    "    test = store_data[store_data[\"is_test\"] == 1]\n",
    "    \n",
    "    #  Use Sales and Promo2Active for modeling\n",
    "    train_ts = train[[\"Sales\", \"Promo2Active\"]].asfreq(\"D\").fillna(method=\"ffill\")\n",
    "    test_ts = test[[\"Sales\", \"Promo2Active\"]].asfreq(\"D\").fillna(method=\"ffill\")\n",
    "    \n",
    "    #  Fit VARMAX Model\n",
    "    try:\n",
    "        model = VARMAX(train_ts, order=(2, 0))\n",
    "        model_fitted = model.fit(disp=False)\n",
    "    except Exception as e:\n",
    "        print(f\" VARMAX failed for Store {store}: {e}\")\n",
    "        continue\n",
    "\n",
    "    #  Forecast\n",
    "    forecast = model_fitted.forecast(steps=len(test_ts))\n",
    "    forecast.index = test_ts.index\n",
    "    forecast.columns = [\"Sales_forecast\", \"Promo2Active_forecast\"]\n",
    "\n",
    "    #  Evaluate MAPE\n",
    "    mape = mean_absolute_percentage_error(test_ts[\"Sales\"], forecast[\"Sales_forecast\"])\n",
    "    print(f\" MAPE for Store {store}: {round(mape * 100, 2)}%\")\n",
    "    \n",
    "    #  Plot Actual vs Forecasted\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(test_ts.index, test_ts[\"Sales\"], label=\"Actual\", color='blue')\n",
    "    plt.plot(test_ts.index, forecast[\"Sales_forecast\"], label=\"Forecast\", color='orange')\n",
    "    plt.title(f\" Forecast vs Actual - Store {store}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Sales\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #  Save results\n",
    "    forecast_results.append({\n",
    "        \"Store\": store,\n",
    "        \"MAPE\": round(mape * 100, 2)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8bf5ae",
   "metadata": {},
   "source": [
    "## 12. Final Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f74762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "\n",
    "result_df = pd.DataFrame(forecast_results)\n",
    "result_df.sort_values(\"MAPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c61b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
